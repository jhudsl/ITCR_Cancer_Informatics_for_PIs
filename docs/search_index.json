[["computing-for-cancer-research.html", "Chapter 3 Computing for Cancer Research 3.1 Computing 3.2 Computing Platforms 3.3 Hardware 3.4 Choosing a Computing Platform 3.5 Local Costs 3.6 Cloud Costs", " Chapter 3 Computing for Cancer Research To afford you the best opportunity to perform the informatics research that you would like, it is useful to become familiar with computing options and costs. This course aims to provide research leaders with some guidance about making decisions for computing hardware (the physical parts of your computing platform) and software (the code that tells the computing platform how to function). It will also describe the benefits and drawbacks of local and “Cloud” computing, as well as the associated costs of each. AVOCADO - This content was adapted from content by Frederick Tan for the ANVil project. See his course created with Jeff Leek, Sarah Wheelan, and Kai Kammers here. 3.1 Computing First we would like to start off with some background about computers. This will better enable you to make decisions about what type of hardware and software you might need for your research. 3.1.1 Core Computing Components There are three core and important components of computers for computing: CPU - the Central Processing Unit The CPU is often called the brain of the computer. It performs and orchestrates computational tasks. Modern computers often perform multiple tasks at once, ranging from 4 tasks on a typical laptop. In addition to the main CPU, computers may be equipped with specialized processors called GPUs which stands for graphics processing units that can perform over 5,000 simpler tasks at once! Memory or RAM - short-term memory RAM stands for Random Access Memory. It is often simply referred to as memory. This short-term memory holds the information that the CPU needs to perform calculations. Since CPUs are fast, RAM needs to be fast, making it relatively expensive. One distinctive feature of memory is that it is temporary. When the electricity is shut off, the data stored in RAM disappears. Storage - long-term memory Storage is also sometimes referred to as long-term memory because electricity is not required to preserve the data. This type of memory is stored using hard disk drives (HDD) also called hard drives or more recently solid-state drives (SSD). Typically ranging from gigabytes to terabytes or more, this type of storage offers big data capacity for a relatively low price at the cost of speed. 3.1.2 Current Computer Capacity So how many tasks can the CPU of an average computer do these days? How much memory and storage do they typically have? These values will probably change very soon, but currently: Laptops can perform 8 CPU tasks at once, storing 64 GB in memory and 8 TB on storage. Handheld tablets can now perform 8 CPU tasks, and store 6 GB in memory and 1 TB on storage. Some phones can compete with laptops from the not so distant past by performing 6 CPU tasks at once and storing 4 GB in memory and 0.5 TB on storage. 3.1.3 Servers What if we need to more computational power than our laptop? You may encounter times where certain informatics tasks take way too long or are not even possible on your personal computer. In terms of hardware, the term server means a computer or computers that can be accessed through a direct local network or the internet to perform computations or store data. Read here to learn more. For example, you could purchase a more powerful computer to act as a server for your lab. Your lab members could connect to this server from their own computers to allow each of them more computational power. Typically computers that act as servers are set up a bit differently than our personal computers, as they do not need the same functionality. For instance they often don’t have capabilities to support a graphical interface (more on what that is later). They are designed to optimize data storage and computational power. However, purchasing a server with enough computational power for your lab may not be as economical, feasible (as having your own server would require that you personally maintain it), or importantly as scalable (you might outgrow your server) as other computing options that we will discuss in a bit. 3.2 Computing Platforms Now that we have discussed a bit about how computers perform computations, lets discuss more about how you might choose your computing platform. A computing platform, is all the hardware (the physical parts of your computing platform) and software (the code that tells the computing platform how to function) necessary to create the environment in which you can perform your computational work. Choosing a computing platform involves both software and hardware decisions. We will focus on hardware. 3.3 Hardware With regards to hardware, there are two major options: Personal computers Shared computers Among shared computers there are three major options: Clusters - institutional or national resources Grids - institutional or national resources the “Cloud” - commercial or national resources 3.3.1 Personal computers These are computers that your lab might own, such as a laptop, a desktop, or a small server used by just your lab. If you are not performing intensive computational tasks, it is possible that you will only need personal computers for your lab. However, you may find that this changes, and you might require connecting your personal computers to shared computers for more computational power and or storage. https://profs.info.uaic.ro/~adria/teach/courses/pcdfeaa/resources/C5_PCD_FEEA_ClusterGridComputing_en.pdf 3.3.2 Shared Computers These are servers (groups of computers) that are shared with other people that you can connect to from your computer (typically using the internet) to help you perform more intensive computational tasks or to store large amounts of data. AVOCADO - maybe put the image here for shared computers - check how anvil images were added within lists Computer Cluster Several of the same type of computer (often in close proximity and connected by a local area network rather than the internet) work together to perform peices of the same single task simultaneously. The idea of performing multiple computations simultaneously is what called parallel computing. There are different designs or architectures for clusters. One common is the Beowulf cluster in which a master computer (called front node or server node) breaks a task up into small pieces that the other computers (called client nodes or simply nodes) perform. For example, if a large file needs to be converted to a different format, pieces of the file will be converted simultaneously by the different nodes. Thus each node is performing the same task just with different pieces of the file. The user has to write code in a special way to specify that they want parallel processing to be used and how. It is important to realize that the CPUs in each of the node computers connected within a cluster are all performing a similar task simultaneously. See here for more information. Computer Grid Different types of computers (often in different locations) work towards an overall common goal by performing different tasks. The concept for grid computing is similar to that of an electric power grid, where only computers (nodes) actively performing a task are using resources. Again, just like computer clusters, there are many types of architectures that can be rather simple to very complex. For example you can think of different universities collaborating to perform different computations for the same project. One university might perform computations using gene expression data about a particular population, while another performs computations using data from another population. Importantly each of these universities might use clusters to perform their specific task. Both grids and clusters use a special type of software called middleware to coordinate the various computers involved. Users need to write their scripts in a way that can be performed by multiple computers simultaneously. Users also need to be conscious of how to schedule their tasks and to follow the rules and etiquette of the specific cluster or grid that they are sharing. See here for more information about the difference between clusters and grids. AVOCADO maybe add as a reference https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1300502 “Cloud” computing] https://go.cloudhealthtech.com/eb-simplify-the-journey-data-center-to-public-cloud.html https://www.geeksforgeeks.org/difference-between-grid-computing-and-cluster-computing/ More recently, commercial “Cloud” solutions are becoming a more viable hardware solution, offered by companies like Amazon, Google, and Microsoft. This option is technically also a shared computer situation. Somewhere these companies have clusters of computers that paying customers use through the internet. In addition there are options like Jetstream which is a more “Cloud-like” part of Xsede. AVOCADO need a new shared computer image 3.3.3 Accessing Shared Computer Resources All of the shared computing options that already exist and that you could utilize involve a data center where are large number of computers are physically housed. Photo by Taylor Vick&gt;on Unsplash You may have access to a HPC (which stands for High Performance Computing) cluster at your institute. Or you could consider using national resources like Xsede. Your university or institution may have a HPC cluster, this means that they have a group of computers acting like servers that people can use to store data or assist with intensive computations. Often institutions can support the cost of many computers within an HPC cluster, allowing for what is called parallel computing. This means that multiple computers will simultaneously perform different parts of the computing required for a given task, thus significantly speeding up the process compared to you trying to perform the task on just your computer! This is also a much more cost effective option than having one expensive supercomputer (a computer that individually has the computational power of many personal computers) to act as a server. It turns out that buying several less powerful computers is cheaper. AVOCADO - update image below to say cluster and grid access Alternatively, you could consider an option like Xsede. Xsede is led by the University of Illinois National Center for Supercomputing Applications (NCSA) and includes 18 other partnering institutions (which are mostly other universities). Through this partnership, they currently support 16 supercomputers. Universities and non-profit researchers in the United States can request access to their computational and data storage resources. Here you can see a photo of Stampede2, one of the supercomputers that members of Xsede can utilize. [source] Stampede2, generously funded by the National Science Foundation (NSF) through award ACI-1134872, is one of the Texas Advanced Computing Center (TACC), University of Texas at Austin’s flagship supercomputers. See here for more information about how you could possibly connect to and utilize Stampede2. Importantly when you use shared computers like national resources like Stampede2 available through Xsede, as well as institutional HPC clusters, you will share these resources with many other people and so you need to learn the proper etiquette for using and sharing these resources. These will vary by the shared resource, however in general: Don’t use all nodes if you don’t need to Don’t use all RAM on a node if you don’t need to Communicate with others if you will be submitting a large or intensive job 3.3.4 Cloud Computing https://go.cloudhealthtech.com/eb-simplify-the-journey-data-center-to-public-cloud.html https://www.geeksforgeeks.org/difference-between-grid-computing-and-cluster-computing/ More recently, commercial “Cloud” solutions are becoming a more viable hardware solution, offered by companies like Amazon, Google, and Microsoft. This option is technically also a shared computer situation. Somewhere these companies have clusters of computers that paying customers use through the internet. In addition there are options like Jetstream wjhich is a more “Cloud-like” part of Xsede. What is the difference between the “Cloud” and other shared computer options? 3.4 Choosing a Computing Platform Choosing a computing platform depends on many different considerations. 3.4.1 Important questions Asking yourself and your research team these questions can help you find the right computing platform: Do I need a graphical interface, a command line interface, or both? What do we mean by this? A graphical interface or graphical user interface or GUI, allows for users to choose functions to perform by interacting with visual representations. They have a “user-centered” design that creates a visual environment where users can for example click on tabs, boxes, or icons for to perform functions. Galaxy offers a graphical interface for performing analyses and tasks. For example in the following image we show a GUI for joining two files: A command line interface (also known as a character interface) allows for software functions to be performed by specifying through commands written in text. This typically offers more control than a graphical interface, however command line interfaces are often less user friendly as they require that the user know the correct commands to use. For example, one could perform functions in R using Bioconductor packages such as Biostrings with a command line interface: A situation where you might use both a command line interface and a GUI, is using RStudio to perform an analysis in R with Bioconductor packages. RStudio is what is called an IDE or an integrated development enviornment, which is an application that supports writing code. There are many tools to help you including a console for writing code in R with command line interfacing, as well as graphical interface tools. As shown in this example below, one can inspect and save a plot (that was created with the command line) by using a GUI. 2) Am I working with protected data that requires special security precautions? If you are working with data that might be protected by HIPAA, such as electronic health records, then special security measures are required to ensure that only authorized users have access to the data. [source] How computationally intensive are my tasks? If you have a large amount of data and/or are performing complex analyses, you may require more computational power than your current laptop can provide. If this is the case, you might consider using a local server or what is called “Cloud” computing (more on that later). How much storage space do I need for both temporary and long-term data? If you are working with large datasets you may also need storage options that go beyond what you currently have available. Local or “Cloud” storage options may work for you, depending on other considerations (security, data transfers) that we will discuss further. Avocado - I want to modify this to highlight the difference between cloud computing by companies vs Xsede Are my local resources sufficient? When a local solution already works, one may rightly question the time required to migrate to the Cloud. However, when local solutions are insufficient or unsustainable, then the Cloud becomes a competitive option. Am I working with especially big or controlled access datasets? Increasingly large datasets like the NCBI Sequence Read Archive are being stored on the Cloud. If your work relies on being able to access the entire dataset, then the Cloud may be your only practical option. Furthermore, if you work with controlled access data, then more platforms are providing compliance with regulations like HIPPA and FedRAMP. Do I need to work with collaborators? Computational research increasingly involves larger and larger collaborations. While many fragmented systems exist to share work, the Cloud presents an opportunity for everyone to share the exact same computational environment including hardware, software, and datasets. If Cloud Computing makes sense for you, then you’re in luck! The past decade has seen the development of many efforts to make Cloud computing, easier, faster, and more affordable. As each platform has their strengths and weaknesses, we will now discuss several opportunities and challenges that Cloud computing presents in the field of computational genomics. 3.4.2 Benefits of Cloud Computing The state of Cloud computing is continually evolving. Here, we highlight three main current benefits: https://jetstream-cloud.org/files/Jetstream-Outreach-C2Exchange-Sep2019.pdf Sharing Workflows The first major benefit is the increasing ease with which one can share and collaborate on research projects. Shown here is the History feature of Galaxy. Using this, one can share not only what datasets they used but also every computational manipulation that was performed. By sharing such a history, one can reproduce an analysis in its entirety, allowing collaborators to offer comments and extend upon the work with ease. Sharing Workflows between Platforms While sharing complete analysis histories is for the most part constrained to a particular software platform, a second benefit that has arisen is the ability to share Workflows between platforms. Shown here is a diagram of a single cell analysis pipeline published by the Klarman Cell Observatory on Dockstore: This higher level abstraction coupled with container technology allows this multistep analysis to be run with relative ease on supporting platforms like Terra or DNAnexus. Using Commodity Hardware The third Benefit we highlight is the increasing ease by which one can provision commodity hardware at scale. What this means is that you can pay reasonable costs to complete your analysis in less time by renting hundreds to tens of thousands of Cloud-based computers – importantly stopping the bill when your analysis is complete. Specialized hardware like GPUs and large memory nodes are also available for rent allowing you to pay only for what you need. 3.4.3 Challenges of Cloud Computing Balancing these three benefits are four challenges: Data Transfer Data transfer and data management remains a cumbersome task. While storing data in the “Cloud” has its advantages, it also has corresponding storage costs. Thus, careful planning is necessary with regards to what data will be stored where, as well as budgeting the time necessary to transfer data back and forth. Data Security Most Cloud resources offer features that make it easier to access and share data, and these features often come at the expense of security. Thus, special precautions must be implemented to securely store protected datasets such as human genome sequences and electronic health records. Costs Controlling costs, especially with regards to storage, presents a third formidable challenge. As many Cloud providers naturally want to encourage usage of their platforms, users must be aware of how much money is currently being spent and be able to project how much money is likely to be spent in the future. We will briefly overview cost controls in the next section. While software platforms can help mitigate these challenges, Cloud computing still incurs costs from the underlying hardware providers. IT A final challenge is that many IT support staff do not have extensive experience managing Cloud resources. Should IT choose to support analysis on the Cloud, they would face the aforementioned challenges of understanding and supporting data management, security compliance, and cost management. Fortunately, large initiatives like AnVIL, Galaxy, and CyVerse continue to work on democratizing access to Cloud computing by tackling many of these challenges. 3.5 Local Costs 3.6 Cloud Costs "]]
